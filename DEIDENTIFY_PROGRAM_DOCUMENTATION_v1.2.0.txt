================================================================================
DE-IDENTIFICATION AND TAGGING PROGRAM - COMPREHENSIVE DOCUMENTATION
Version: 1.2.0
Date: Current Session
================================================================================

TABLE OF CONTENTS
================================================================================
1. PROJECT OVERVIEW
2. PROBLEM STATEMENT
3. SOLUTION ARCHITECTURE
4. TECHNICAL IMPLEMENTATION
5. VERSION HISTORY AND IMPROVEMENTS
6. CURRENT STATE
7. USAGE INSTRUCTIONS
8. FUTURE ENHANCEMENTS
9. LESSONS LEARNED
10. RELATED FILES AND DEPENDENCIES

================================================================================
1. PROJECT OVERVIEW
================================================================================

PROJECT CONTEXT:
This project is part of the TCRGP II (Tribal Cooperative Research Group Project II)
Analysis, which studies Indigenous cooperatives and tribal-related entities. The
research involves analyzing interview transcripts from cooperative leaders and
community members to understand:
- How cooperatives are structured and governed
- How they align with tribal values and traditional systems
- Their financial, operational, and cultural characteristics
- Their responses to challenges (including COVID-19)
- Their success factors and outcomes

DATA SOURCES:
- Round 1 Interviews: 4 cooperatives (E' Numu Diip/Allottees, River Select Foods,
  Many Nations Cooperative, RTZ Artists Cooperative)
- Round 2 Interviews: 3 entities (Alaska Village Electric Coop, Earth and Sky
  Floral, TWU)
- Newer Transcripts: 6 additional tribal-related entities (including TIWA Lending
  Services, Ohelaku Among the Cornstalks, Umuha Nation Public Schools, San Xavier
  Coop, CSVANW)

ANALYSIS FRAMEWORK:
The project uses a standardized framework of 44 quantitative metrics across 11
categories:
1. Membership
2. Governance
3. Finance
4. Employment
5. Partnerships
6. Innovation
7. Operations
8. Markets
9. Technology
10. Culture
11. Outcomes

Additionally, interviews are analyzed against 9 standardized survey questions to
assess qualitative coverage.

================================================================================
2. PROBLEM STATEMENT
================================================================================

ORIGINAL PROBLEM:
When new interview transcripts are received, they need to be:
1. De-identified for privacy protection (names, locations, sensitive data)
2. Tagged with research-relevant keywords for analysis
3. Processed consistently with existing data
4. Handled efficiently without manual work

SPECIFIC CHALLENGES:
- Transcripts contain sensitive information (names, organizations, locations,
  financial amounts, specific dates)
- Transcripts often have transcription errors and misspellings (e.g., "UPIC" for
  "Yupik", "Burshia" variants)
- Transcripts may be in raw format (WEBVTT with timestamps) or cleaned format
- Need to maintain traceability (mapping files) for research validation
- Need to extract quantitative metrics and qualitative themes
- Need to align with existing research categories and survey questions

QUALITY REQUIREMENTS:
- Accurate entity extraction (avoid false positives)
- Proper handling of name variants and misspellings
- Clean, readable output format
- Comprehensive keyword tagging
- Secure mapping files for de-identification codes

================================================================================
3. SOLUTION ARCHITECTURE
================================================================================

PROGRAM: deidentify_and_tag_transcripts_v1.2.0.py

CORE COMPONENTS:

1. NAME VARIANT DETECTION ENGINE
   - Detects and clusters name variants accounting for misspellings
   - Uses fuzzy matching (75% similarity threshold)
   - Handles known misspellings (e.g., "burshia" variants)
   - Creates canonical mappings for consistent de-identification

2. DE-IDENTIFICATION ENGINE
   - Extracts entities (persons, organizations, locations, tribes)
   - Creates unique codes (Person_1, Organization_1, Location_1, etc.)
   - Replaces entities with codes in text
   - Handles financial amounts and years
   - Removes timestamps from WEBVTT format
   - Corrects common misspellings

3. KEYWORD TAGGING ENGINE
   - Tags research-relevant keywords across 15 categories
   - Aligns with 9 survey questions
   - Identifies Indigenous-specific terminology
   - Extracts quantitative metrics (members, employees, dollar amounts, etc.)

4. TEXT PROCESSING UTILITIES
   - Extracts text from DOCX files
   - Removes WEBVTT timestamps
   - Corrects misspellings
   - Formats as clean dialogue

OUTPUT FILES:
For each transcript processed, generates:
1. {filename}_deidentified.txt - De-identified transcript
2. {filename}_mapping.json - Secure mapping of codes to original entities
3. {filename}_tags.csv - All keyword tags with line numbers and context
4. processing_summary.json - Overall statistics

================================================================================
4. TECHNICAL IMPLEMENTATION
================================================================================

KEY TECHNICAL DECISIONS:

1. ENTITY EXTRACTION STRATEGY
   - VERY RESTRICTIVE: Only extracts actual proper nouns
   - Validates names/locations before extraction
   - Filters out common phrases aggressively
   - Uses specific patterns for each entity type

2. NAME VALIDATION
   - Must be 2-4 words (first name + last name)
   - All words must start with capital letter
   - Excludes common phrases ("not able", "that you", etc.)
   - Excludes common words ("the", "a", "project", "future", etc.)

3. LOCATION VALIDATION
   - Must start with capital letter
   - Excludes common phrases ("where the", "are here", etc.)
   - Includes known cities/states explicitly
   - Only extracts actual place names

4. FUZZY MATCHING
   - Uses SequenceMatcher for similarity calculation
   - 75% similarity threshold for name variants
   - Handles misspellings and transcription errors

5. TIMESTAMP REMOVAL
   - Removes WEBVTT header
   - Removes timestamp lines (00:00:01.900 --> 00:00:12.490)
   - Removes segment numbers
   - Cleans up multiple newlines

6. MISSPELLING CORRECTION
   - Dictionary-based corrections (UPIC → Yupik)
   - Applied before entity extraction
   - Uses word boundaries for whole-word replacement

7. DIALOGUE FORMATTING
   - Identifies speaker labels (Person_X:)
   - Maps to Interviewer/Interviewee roles
   - Formats as clean dialogue

CONFIGURATION:

RESEARCH_CATEGORIES: 15 categories with keywords
- Membership, Governance, Finance, Employment, Partnerships, Innovation,
  Operations, Markets, Technology, Culture, Geography, Risk, Timeline,
  Success, COVID

SURVEY_QUESTION_TAGS: 9 survey questions with keywords
- Q1: Tribal values and traditional systems
- Q2: Marketing plan
- Q3: Website/social media
- Q4: Outside assistance
- Q5: Standard cooperative approaches
- Q6: Community differences
- Q7: Leadership engagement
- Q8: Success
- Q9: COVID impact

INDIGENOUS_TERMS: Specific terminology
- sovereignty, self-determination, matriarch, elder, ceremony, traditional
  knowledge, land-based, water rights, treaty, reservation, pueblo, nation,
  tribal council, indigenous, native, first nation, aboriginal

COMMON_MISSPELLINGS: Known transcription errors
- burshia: ["brache", "berchet", "berchet-gowazi", "brochure", "burche"]
- jodi: ["jody"]
- pamela: ["pam"]
- standing: ["stand"]

MISSPELLING_CORRECTIONS: Direct corrections
- upic/yupic/UPIC/YUPIC → Yupik

EXCLUDED_WORDS: Common phrases to filter out
- persons: {"some", "project", "the future", "not able to answer them
  immediately", "that you", "like you", "sure that", "looking at my",
  "asking if"}
- locations: {"some", "project", "the future", "information", "things",
  "stuff", "question", "type", "them", "need", "makes", "on the", "where the",
  "are here", "is because", "is included", "were happening"}
- organizations: {"the cooperative", "a cooperative", "this co-op", "the
  co-op", "your cooperative", "our cooperative", "their co-op"}

SIMILARITY_THRESHOLD: 0.75 (75% similarity for fuzzy matching)

================================================================================
5. VERSION HISTORY AND IMPROVEMENTS
================================================================================

VERSION 1.0.0 (Initial Release):
- Basic de-identification (names, locations, organizations, tribes)
- Name variant detection with fuzzy matching
- Keyword tagging across research categories
- Quantitative metric extraction
- Mapping file generation
- Tag CSV generation

ISSUES IDENTIFIED IN v1.0.0:
- Over-extraction of entities (405 locations, 25 persons with many false
  positives)
- No timestamp removal
- No misspelling correction
- No formatting improvements
- Extracted common phrases as entities ("not able to answer them immediately"
  → Person_4, "some", "project", "the future" → Locations)

VERSION 1.1.0 (First Improvement):
- Added timestamp removal functionality
- Added misspelling correction (UPIC → Yupik)
- Improved entity extraction patterns
- Better formatting output
- Filters for common words

ISSUES REMAINING IN v1.1.0:
- Still over-extracting entities (though improved)
- Some false positives still present
- Entity validation not strict enough

VERSION 1.2.0 (Current - Major Improvement):
- MUCH more restrictive entity extraction
- Added validation functions (is_valid_name, is_valid_location)
- Only extracts actual proper nouns
- Filters out common phrases aggressively
- Results: 3 persons (was 25), 29 locations (was 405), 4 orgs (was 52)
- Clean dialogue formatting
- Timestamp removal
- Misspelling correction

KEY IMPROVEMENTS IN v1.2.0:
1. is_valid_name() function:
   - Must be 2-4 words
   - All words must start with capital letter
   - Excludes common phrases and words
   - Validates against excluded words list

2. is_valid_location() function:
   - Must start with capital letter
   - Excludes common phrases
   - Validates against excluded words list

3. Restrictive extraction patterns:
   - Persons: Only speaker labels at start of line
   - Organizations: Only specific org types with proper capitalization
   - Locations: Only actual place names (cities, states, reservations)
   - Tribes: Only with Nation/Tribe/Pueblo suffix

4. Known location list:
   - Explicitly includes known cities (Bethel, Anchorage, Phoenix, etc.)
   - Explicitly includes states (Alaska, Arizona, New Mexico, etc.)

================================================================================
6. CURRENT STATE
================================================================================

CURRENT VERSION: v1.2.0

FILE LOCATION:
/Users/gregoryconner/Dropbox/Cursor/TCRGP II Analysis/deidentify_and_tag_transcripts_v1.2.0.py

OUTPUT DIRECTORY:
/Users/gregoryconner/Dropbox/Cursor/TCRGP II Analysis/deidentified_transcripts/

PROCESSED TRANSCRIPTS:
- Alaska Village Electric Coop Recording.transcript_Anna Sattler_10.10.2025.docx
- GMT20250813-152303_Recording.transcript_...TIWA LENDING SERVICES.docx
- Raw Recording Transcript - Lea Zeise - Ohelaku Among the Cornstalks 12.17.25.docx
- Raw transcript - Umuha Nation Public Schools - Ricardo Ariza from _10.24.25.docx
- San Xavier Coop Transcript - Duran Andrews 11.10.2025.docx
- The Coalition to Stop Violence Against Native Women CSVANW N Michelena 8 4 25.docx

PERFORMANCE METRICS (Alaska Electric example):
- Original: 89,057 characters
- Entities found: 3 persons, 4 orgs, 29 locations, 2 tribes
- Tags found: 292 keyword matches across 24 categories
- False positives: Eliminated (was 25 persons, 405 locations)

QUALITY COMPARISON TO REFERENCE CLEANED VERSION:
✓ Timestamps removed
✓ Misspellings fixed (Yupik correct)
✓ Clean dialogue format (Interviewer/Interviewee)
✓ Much better entity extraction (no false positives)
⚠ Could add header/metadata (optional enhancement)
⚠ Could add explanatory annotations (optional enhancement)

================================================================================
7. USAGE INSTRUCTIONS
================================================================================

BASIC USAGE:

1. Place transcript files in "newer transcripts" directory
   - Supports .docx and .txt files
   - Files should be raw or cleaned transcripts

2. Run the program:
   python3 deidentify_and_tag_transcripts_v1.2.0.py

3. Output files are created in "deidentified_transcripts" directory:
   - {filename}_deidentified.txt - Clean, de-identified transcript
   - {filename}_mapping.json - Secure mapping (keep private!)
   - {filename}_tags.csv - All keyword tags
   - processing_summary.json - Overall statistics

DEPENDENCIES:
- Python 3.x
- python-docx (for DOCX file reading)
  Install with: pip install python-docx

INPUT DIRECTORY STRUCTURE:
TCRGP II Analysis/
  ├── newer transcripts/          # Input directory
  │   ├── transcript1.docx
  │   ├── transcript2.txt
  │   └── ...
  └── deidentified_transcripts/   # Output directory (created automatically)
      ├── transcript1_deidentified.txt
      ├── transcript1_mapping.json
      ├── transcript1_tags.csv
      └── processing_summary.json

OUTPUT FILE FORMATS:

1. DEIDENTIFIED.TXT:
   - Clean dialogue format
   - Interviewer: / Interviewee: labels
   - No timestamps
   - Misspellings corrected
   - Entities replaced with codes

2. MAPPING.JSON:
   {
     "source_file": "filename.docx",
     "persons": {
       "Pamela Standing": "Person_1",
       "Anna Sattler": "Person_2",
       ...
     },
     "organizations": {...},
     "locations": {...},
     "tribes": {...},
     "name_variants": {...}
   }

3. TAGS.CSV:
   - Columns: Tag_Category, Line_Number, Matched_Text, Context
   - One row per keyword match
   - Includes line numbers for traceability

4. PROCESSING_SUMMARY.JSON:
   - Overall statistics
   - Entities found per file
   - Tags found per file
   - Total counts

================================================================================
8. FUTURE ENHANCEMENTS
================================================================================

OPTIONAL IMPROVEMENTS:

1. HEADER GENERATION:
   - Add interview metadata header (like reference cleaned version)
   - Include: Interview title, participants, date
   - Format: "Alaska Village Electric Coop (AVEC) - Anna Sattler
             Interviewer: Jodi Burshia, Pamela Standing
             10/10/2025"

2. EXPLANATORY ANNOTATIONS:
   - Add [bracketed] annotations for context
   - Example: [brackets] for clarifications
   - Similar to reference cleaned version

3. SPEAKER IDENTIFICATION:
   - Better detection of interviewer vs interviewee
   - Could use first person to identify interviewee
   - Could use question patterns to identify interviewer

4. ADDITIONAL MISSPELLINGS:
   - Expand misspelling dictionary based on new transcripts
   - Learn from processing errors

5. ORGANIZATION DETECTION:
   - Improve organization extraction
   - Handle acronyms better (AVEC, AVAC, etc.)
   - Better handling of cooperative variations

6. LOCATION DETECTION:
   - Expand known location list
   - Better handling of reservation names
   - Handle multi-word locations better

7. QUANTITATIVE METRICS:
   - Extract more metric types
   - Better number recognition
   - Handle ranges and approximations

8. QUALITY VALIDATION:
   - Add validation checks
   - Flag potential issues
   - Suggest corrections

9. BATCH PROCESSING:
   - Process multiple directories
   - Compare across transcripts
   - Generate comparative reports

10. INTEGRATION:
    - Integrate with existing analysis scripts
    - Link to quantitative matrix generation
    - Connect to white paper generation

================================================================================
9. LESSONS LEARNED
================================================================================

KEY INSIGHTS:

1. ENTITY EXTRACTION IS TRICKY:
   - Initial approach was too permissive
   - Common phrases were extracted as entities
   - Need strict validation functions
   - Better to under-extract than over-extract

2. VALIDATION IS CRITICAL:
   - is_valid_name() and is_valid_location() functions essential
   - Must filter out common words and phrases
   - Must check capitalization patterns
   - Must validate against excluded words list

3. PATTERN MATCHING LIMITATIONS:
   - Regex patterns alone not sufficient
   - Need validation layer on top
   - Context matters (speaker labels vs. regular text)

4. FUZZY MATCHING HELPS:
   - Handles transcription errors well
   - 75% threshold works for name variants
   - Need canonical mapping for consistency

5. FORMATTING MATTERS:
   - Clean output improves usability
   - Dialogue format is readable
   - Timestamp removal essential for raw transcripts

6. TRACEABILITY IMPORTANT:
   - Mapping files enable re-identification if needed
   - Line numbers in tags enable verification
   - JSON format enables programmatic access

7. ITERATIVE IMPROVEMENT:
   - v1.0.0 had major issues (over-extraction)
   - v1.1.0 improved but still had problems
   - v1.2.0 achieved quality goals
   - Each version built on previous learnings

8. REFERENCE COMPARISON:
   - Comparing to reference cleaned version was valuable
   - Identified missing features
   - Validated quality improvements

================================================================================
10. RELATED FILES AND DEPENDENCIES
================================================================================

RELATED SCRIPTS:

1. extract_docx_text.py
   - Utility for extracting text from DOCX files
   - Used by de-identification program
   - Can be run standalone for testing

2. analyze_round2_interviews_v1.0.0.py
   - Analyzes Round 2 interviews for survey question coverage
   - Uses keyword matching
   - Generates coverage reports

3. extract_interview_quotes_v1.0.0.py
   - Extracts direct quotes from interviews
   - Organized by survey question
   - Generates professional spreadsheets and PDFs

4. create_comprehensive_white_paper_pdf_v1.0.0.py
   - Generates comprehensive white paper PDF
   - Combines analysis from different rounds
   - Professional formatting

5. add_interview_citations.py
   - Adds interview position citations to spreadsheets
   - Maps data points to specific line numbers
   - Creates citation format: [Interview L##]

DATA FILES:

1. conner_human_readable_matrix_v0.2.1.csv
   - Quantitative metrics from Round 1 interviews
   - Baseline for comparative analysis
   - 44 metrics across 11 categories

2. interview_quotes_by_question_v1.0.0.csv/xlsx/pdf
   - Direct quotes organized by survey question
   - Evidence for qualitative analysis
   - Professional formatting

3. TCGRPII survey questions 11:14:25.txt
   - 9 standardized survey questions
   - Used for coverage analysis
   - Reference for tagging

REFERENCE FILES:

1. older transcripts/Combined Sections Alaska Village Electric Coop Recording...
   - Reference cleaned version
   - Shows expected output format
   - Used for quality comparison

2. older transcripts/alaska_interview.txt
   - Text version of Alaska interview
   - Used for analysis

3. older transcripts/earth_sky_interview.txt
   - Earth and Sky Floral interview
   - Used for analysis

DOCUMENTATION FILES:

1. DEIDENTIFY_README_v1.0.0.txt
   - Initial documentation for de-identification program
   - Basic usage instructions

2. INTERVIEW_QUOTES_README_v1.0.0.txt
   - Documentation for quote extraction process
   - Methodology and deliverables

3. COMPREHENSIVE_WHITE_PAPER_README_v1.0.0.txt
   - Documentation for white paper generation
   - Project overview and structure

4. VERSION
   - Tracks current version numbers
   - Updated after each version change

DIRECTORY STRUCTURE:

TCRGP II Analysis/
  ├── newer transcripts/              # Input for de-identification
  ├── older transcripts/              # Reference files
  ├── deidentified_transcripts/      # Output from de-identification
  ├── deidentify_and_tag_transcripts_v1.2.0.py  # Main program
  ├── extract_docx_text.py          # Utility script
  ├── analyze_round2_interviews_v1.0.0.py
  ├── extract_interview_quotes_v1.0.0.py
  ├── create_comprehensive_white_paper_pdf_v1.0.0.py
  ├── conner_human_readable_matrix_v0.2.1.csv
  ├── interview_quotes_by_question_v1.0.0.*
  ├── TCGRPII survey questions 11:14:25.txt
  └── Documentation files (*.txt, *.md)

================================================================================
11. TECHNICAL DETAILS FOR CONTINUATION
================================================================================

CODE STRUCTURE:

1. CONFIGURATION SECTION (lines ~24-76):
   - RESEARCH_CATEGORIES: 15 research categories with keywords
   - SURVEY_QUESTION_TAGS: 9 survey questions with keywords
   - INDIGENOUS_TERMS: Indigenous-specific terminology
   - COMMON_MISSPELLINGS: Known transcription errors
   - MISSPELLING_CORRECTIONS: Direct corrections
   - EXCLUDED_WORDS: Common phrases to filter
   - SIMILARITY_THRESHOLD: 0.75 for fuzzy matching

2. UTILITY FUNCTIONS (lines ~78-110):
   - similarity(): Calculate string similarity
   - find_similar_names(): Find similar names
   - extract_text_from_docx(): Extract from DOCX
   - remove_webvtt_timestamps(): Remove timestamps
   - correct_misspellings(): Fix misspellings
   - format_as_dialogue(): Format as dialogue

3. NAME VARIANT DETECTION (lines ~198-258):
   - NameVariantDetector class
   - add_name(): Add name with fuzzy matching
   - get_canonical_mapping(): Get variant mappings

4. DE-IDENTIFICATION ENGINE (lines ~260-450):
   - DeIdentifier class
   - is_valid_name(): Validate person names
   - is_valid_location(): Validate location names
   - extract_entities(): Extract with strict filtering
   - create_codes(): Create de-identification codes
   - deidentify_text(): Replace entities with codes

5. KEYWORD TAGGING (lines ~452-520):
   - KeywordTagger class
   - tag_text(): Tag keywords with line numbers

6. MAIN PROCESSING (lines ~522-580):
   - process_transcript(): Process single file
   - main(): Main execution

KEY FUNCTIONS TO MODIFY:

1. To add new research categories:
   - Edit RESEARCH_CATEGORIES dictionary
   - Add category name and keyword list

2. To add new misspellings:
   - Edit COMMON_MISSPELLINGS dictionary
   - Add canonical form and variants

3. To improve entity extraction:
   - Modify extract_entities() method
   - Adjust validation functions (is_valid_name, is_valid_location)
   - Update EXCLUDED_WORDS lists

4. To change output format:
   - Modify deidentify_text() method
   - Adjust format_as_dialogue() function
   - Add header generation if needed

TESTING APPROACH:

1. Test with known transcripts:
   - Use Alaska Electric interview (has reference cleaned version)
   - Compare output to reference
   - Check entity extraction accuracy

2. Validate entity extraction:
   - Check mapping.json for false positives
   - Verify only actual proper nouns extracted
   - Confirm no common phrases included

3. Check formatting:
   - Verify timestamps removed
   - Check misspellings corrected
   - Validate dialogue format

4. Test with new transcripts:
   - Process new files
   - Check for errors
   - Validate output quality

DEBUGGING TIPS:

1. If over-extraction occurs:
   - Check EXCLUDED_WORDS lists
   - Review is_valid_name() and is_valid_location()
   - Add problematic phrases to exclusion lists

2. If entities missed:
   - Check extraction patterns
   - Verify capitalization
   - Review validation functions

3. If formatting issues:
   - Check timestamp removal regex
   - Verify dialogue formatting logic
   - Review speaker identification

4. If performance issues:
   - Check file sizes
   - Review regex patterns (may be slow)
   - Consider optimization if needed

================================================================================
12. INTEGRATION WITH EXISTING WORKFLOW
================================================================================

CURRENT WORKFLOW:

1. Receive new transcripts → Place in "newer transcripts" directory
2. Run deidentify_and_tag_transcripts_v1.2.0.py
3. Review deidentified_transcripts/ output
4. Use deidentified files for analysis
5. Use tags.csv for keyword analysis
6. Use mapping.json for traceability (keep secure!)

INTEGRATION POINTS:

1. WITH QUANTITATIVE ANALYSIS:
   - De-identified transcripts can be analyzed for metrics
   - Tags can help identify relevant sections
   - Mapping files enable citation if needed

2. WITH QUALITATIVE ANALYSIS:
   - Tags align with survey questions
   - Can extract quotes from de-identified files
   - Line numbers enable precise citations

3. WITH WHITE PAPER GENERATION:
   - De-identified transcripts can be included
   - Tags can inform chapter structure
   - Metrics can be extracted for tables

4. WITH COMPARATIVE ANALYSIS:
   - All transcripts processed consistently
   - Tags enable cross-transcript analysis
   - Metrics can be compared across entities

NEXT STEPS IN WORKFLOW:

1. Process all new transcripts with v1.2.0
2. Review and validate output quality
3. Integrate with existing analysis scripts
4. Generate updated matrices (Round 1 + Round 2 + New)
5. Create comparative reports
6. Update white paper with new findings

================================================================================
END OF DOCUMENTATION
================================================================================

This documentation should enable continuation of the work in future sessions.
For questions or issues, refer to:
- Code comments in deidentify_and_tag_transcripts_v1.2.0.py
- Reference cleaned version in older transcripts/
- Related documentation files
- Version history in this document

Last Updated: Current Session
Version: 1.2.0

